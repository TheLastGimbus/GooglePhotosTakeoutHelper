#!/usr/bin/env bash
# shellcheck source=/dev/null

###################################################
# Download a gdrive file
# Todo: write doc
###################################################
_download_file() {
    [[ $# -lt 3 ]] && printf "%s: Missing arguments\n" "${FUNCNAME[0]}" && return 1
    declare file_id="${1}" name="${2}" server_size="${3}" parallel="${4}" \
        range downloaded old_downloaded left speed eta \
        use_aria="${DOWNLOAD_WITH_ARIA}" flag flag_value url cookies

    server_size_readable="$(_bytes_to_human "${server_size}")"
    _print_center "justify" "${name}" " | ${server_size:+${server_size_readable}}" "="

    if [[ -s ${name} ]]; then
        declare local_size && local_size="$(_actual_size_in_bytes "${name}")"

        if [[ ${local_size} -ge "${server_size}" ]]; then
            "${QUIET:-_print_center}" "justify" "File already present" "=" && _newline "\n"
            _log_in_file
            return 0
        else
            _print_center "justify" "File is partially" " present, resuming.." "-"
            range="Range: bytes=${local_size}-${server_size}"
            # aria can only resume downloads if either oauth or api key download method is used.
            [[ -z ${OAUTH_ENABLED:-${API_KEY_DOWNLOAD}} ]] && unset use_aria
        fi
    else
        [[ "${server_size}" -gt 0 ]] && range="Range: bytes=0-${server_size}"
        _print_center "justify" "Downloading file.." "-"
    fi

    # download with oauth creds if enabled
    if [[ -n ${OAUTH_ENABLED} ]]; then
        . "${TMPFILE}_ACCESS_TOKEN"
        flag="--header" flag_value="Authorization: Bearer ${ACCESS_TOKEN}"
        url="${API_URL}/drive/${API_VERSION}/files/${file_id}?alt=media&supportsAllDrives=true&includeItemsFromAllDrives=true"
    elif [[ -n ${API_KEY_DOWNLOAD} ]]; then
        # download with api key
        flag="--referer" flag_value="https://drive.google.com"
        url="${API_URL}/drive/${API_VERSION}/files/${file_id}?alt=media&supportsAllDrives=true&includeItemsFromAllDrives=true&key=${API_KEY}"
    else
        # normal downloading
        "${EXTRA_LOG}" "justify" "Fetching" " cookies.." "-"
        # shellcheck disable=SC2086
        curl -c "${TMPFILE}_${file_id}_COOKIE" -I ${CURL_PROGRESS} -o /dev/null "https://drive.google.com/uc?export=download&id=${file_id}" || :
        for _ in 1 2; do _clear_line 1; done
        confirm_string="$(: "$(grep -F 'download_warning' "${TMPFILE}_${file_id}_COOKIE")" && printf "%s\n" "${_//*$'\t'/}")" || :

        flag="-b" flag_value="${TMPFILE}_${file_id}_COOKIE"

        # aria need some elements removed from cookies when it is generated by curl
        [[ -n ${use_aria} ]] && {
            cookies="$(sed -e "s/^\# .*//g" -e "s/^\#HttpOnly_//g" "${TMPFILE}_${file_id}_COOKIE")"
            printf "%s\n" "${cookies}" >| "${TMPFILE}_${file_id}_COOKIE"
            flag="--load-cookies"
        }

        url="https://drive.google.com/uc?export=download&id=${file_id}${confirm_string:+&confirm=${confirm_string}}"
    fi

    if [[ -n ${use_aria} ]]; then
        # shellcheck disable=SC2086
        aria2c ${SPEED_LIMIT:+${ARIA_SPEED_LIMIT_FLAG}} ${SPEED_LIMIT} ${ARIA_EXTRA_FLAGS} \
            "${flag}" "${flag_value}" \
            "${url}" -o "${name}" &
        pid="${!}"
    else
        # shellcheck disable=SC2086
        curl ${SPEED_LIMIT:+${CURL_SPEED_LIMIT_FLAG}} ${SPEED_LIMIT} ${CURL_EXTRA_FLAGS} \
            --header "${range}" \
            "${flag}" "${flag_value}" \
            "${url}" >> "${name}" &
        pid="${!}"
    fi

    if [[ -n ${parallel} ]]; then
        wait "${pid}" 2>| /dev/null 1>&2
    else
        until [[ -f ${name} ]] || ! kill -0 "${pid}" 2>| /dev/null 1>&2; do sleep 0.5; done

        _newline "\n\n"
        until ! kill -0 "${pid}" 2>| /dev/null 1>&2; do
            downloaded="$(_actual_size_in_bytes "${name}")"
            left="$((server_size - downloaded))"
            speed="$((downloaded - old_downloaded))"
            { [[ ${speed} -gt 0 ]] && eta="$(_display_time "$((left / speed))")"; } || eta=""
            sleep 0.5
            _move_cursor 2
            ##################################################### Amount Downloaded ####################### Amount left to download ##################
            _print_center "justify" "Downloaded: $(_bytes_to_human "${downloaded}") " "| Left: $(_bytes_to_human "${left}")" "="
            ########################################### Speed of download ############### ETA ######################
            _print_center "justify" "Speed: $(_bytes_to_human "${speed}")/s " "| ETA: ${eta:-Unknown}" "-"
            old_downloaded="${downloaded}"
        done
    fi

    if [[ $(_actual_size_in_bytes "${name}") -ge "${server_size}" ]]; then
        for _ in 1 2 3; do _clear_line 1; done
        "${QUIET:-_print_center}" "justify" "Downloaded" "=" && _newline "\n"
        rm -f "${name}.aria2"
    else
        "${QUIET:-_print_center}" "justify" "Error: Incomplete" " download." "=" 1>&2
        return 1
    fi
    _log_in_file "${name}" "${server_size_readable}" "${file_id}"
    return 0
}

###################################################
# A extra wrapper for _download_file function to properly handle retries
# also handle uploads in case downloading from folder
# Todo: write doc
###################################################
_download_file_main() {
    [[ $# -lt 2 ]] && printf "%s: Missing arguments\n" "${FUNCNAME[0]}" 1>&2 && return 1
    declare line fileid name size parallel retry="${RETRY:-0}" _sleep && unset RETURN_STATUS
    [[ ${1} = parse ]] && parallel="${3}" line="${2}" fileid="${line%%"|:_//_:|"*}" \
        name="${line##*"|:_//_:|"}" size="$(_tmp="${line#*"|:_//_:|"}" && printf "%s\n" "${_tmp%"|:_//_:|"*}")"
    parallel="${parallel:-${5}}"

    unset RETURN_STATUS && until [[ ${retry} -le 0 && -n ${RETURN_STATUS} ]]; do
        if [[ -n ${parallel} ]]; then
            _download_file "${fileid:-${2}}" "${name:-${3}}" "${size:-${4}}" true 2>| /dev/null 1>&2 && RETURN_STATUS=1 && break
        else
            _download_file "${fileid:-${2}}" "${name:-${3}}" "${size:-${4}}" && RETURN_STATUS=1 && break
        fi
        sleep "$((_sleep += 1))" # on every retry, sleep the times of retry it is, e.g for 1st, sleep 1, for 2nd, sleep 2
        RETURN_STATUS=2 retry="$((retry - 1))" && continue
    done
    { [[ ${RETURN_STATUS} = 1 ]] && printf "%b" "${parallel:+${RETURN_STATUS}\n}"; } || printf "%b" "${parallel:+${RETURN_STATUS}\n}" 1>&2
    return 0
}

###################################################
# Download a gdrive folder along with sub folders
# File IDs are fetched inside the folder, and then downloaded seperately.
# Todo: write doc
###################################################
_download_folder() {
    [[ $# -lt 2 ]] && printf "%s: Missing arguments\n" "${FUNCNAME[0]}" && return 1
    declare folder_id="${1}" name="${2}" parallel="${3}"
    declare json_search json_search_fragment next_page_token \
        error_status success_status files=() folders=() \
        files_size files_name files_list num_of_files folders_list num_of_folders
    _newline "\n"
    "${EXTRA_LOG}" "justify" "${name}" "="
    "${EXTRA_LOG}" "justify" "Fetching folder" " details.." "-"

    _search_error_message() {
        "${QUIET:-_print_center}" "justify" "Error: Cannot" ", fetch folder details." "="
        printf "%s\n" "${1:?}" && return 1
    }

    # do the first request with pagesize 1000, and fetch nextPageToken
    if json_search="$("${API_REQUEST_FUNCTION}" "files?q=%27${folder_id}%27+in+parents&fields=nextPageToken,files(name,size,id,mimeType)&pageSize=1000&orderBy=name")"; then
        # fetch next page jsons till nextPageToken is available
        until ! next_page_token="$(_json_value nextPageToken 1 1 <<< "${json_search_fragment:-${json_search}}")"; do
            json_search_fragment="$("${API_REQUEST_FUNCTION}" "files?q=%27${folder_id}%27+in+parents&fields=nextPageToken,files(name,size,id,mimeType)&pageSize=1000&orderBy=name&pageToken=${next_page_token}")" ||
                _search_error_message "${json_search_fragment}"

            # append the new fetched json to initial json
            json_search="${json_search}
${json_search_fragment}"
        done
    else
        # error message in case some thing goes wrong
        _search_error_message "${json_search}"
    fi && _clear_line 1

    # parse the fetched json and make a list containing files size, name and id
    "${EXTRA_LOG}" "justify" "Preparing files list.." "="
    mapfile -t files <<< "$(printf "%s\n" "${json_search}" | grep '"size":' -B3 | _json_value id all all)" || :
    files_size="$(_json_value size all all <<< "${json_search}")" || :
    files_name="$(printf "%s\n" "${json_search}" | grep size -B2 | _json_value name all all)" || :
    files_list="$(while read -r -u 4 _id && read -r -u 5 _size && read -r -u 6 _name; do
        printf "%s\n" "${_id}|:_//_:|${_size}|:_//_:|${_name}"
    done 4<<< "$(printf "%s\n" "${files[@]}")" 5<<< "${files_size}" 6<<< "${files_name}")"
    _clear_line 1

    # parse the fetched json and make a list containing sub folders name and id
    "${EXTRA_LOG}" "justify" "Preparing sub folders list.." "="
    mapfile -t folders <<< "$(printf "%s\n" "${json_search}" | grep '"mimeType":.*folder.*' -B2 | _json_value id all all)" || :
    folders_name="$(printf "%s\n" "${json_search}" | grep '"mimeType":.*folder.*' -B1 | _json_value name all all)" || :
    folders_list="$(while read -r -u 4 _id && read -r -u 5 _name; do
        printf "%s\n" "${_id}|:_//_:|${_name}"
    done 4<<< "$(printf "%s\n" "${folders[@]}")" 5<<< "${folders_name}")"
    _clear_line 1

    for _ in 1 2; do _clear_line 1; done

    [[ -z ${files[*]:-${folders[*]}} ]] && _print_center "justify" "${name}" " | Empty Folder" "=" && _newline "\n" && return 0

    [[ -n ${files[*]} ]] && num_of_files="${#files[@]}"
    [[ -n ${folders[*]} ]] && num_of_folders="${#folders[@]}"

    _print_center "justify" "${name}" "${num_of_files:+ | ${num_of_files} files}${num_of_folders:+ | ${num_of_folders} sub folders}" "=" && _newline "\n\n"

    if [[ -f ${name} ]]; then
        name="${name}${RANDOM}"
    fi && mkdir -p "${name}"

    cd "${name}" 2>| /dev/null 1>&2 || exit 1

    if [[ -n "${num_of_files}" ]]; then
        if [[ -n ${parallel} ]]; then
            NO_OF_PARALLEL_JOBS_FINAL="$((NO_OF_PARALLEL_JOBS > num_of_files ? num_of_files : NO_OF_PARALLEL_JOBS))"

            [[ -f "${TMPFILE}"SUCCESS ]] && rm "${TMPFILE}"SUCCESS
            [[ -f "${TMPFILE}"ERROR ]] && rm "${TMPFILE}"ERROR

            # shellcheck disable=SC2016
            printf "%s\n" "${files_list}" | xargs -n1 -P"${NO_OF_PARALLEL_JOBS_FINAL}" -i bash -c '
                _download_file_main parse "{}" true
            ' 1>| "${TMPFILE}"SUCCESS 2>| "${TMPFILE}"ERROR &
            pid="${!}"

            until [[ -f "${TMPFILE}"SUCCESS || -f "${TMPFILE}"ERROR ]]; do sleep 0.5; done

            _clear_line 1
            until ! kill -0 "${pid}" 2>| /dev/null 1>&2; do
                success_status="$(_count < "${TMPFILE}"SUCCESS)"
                error_status="$(_count < "${TMPFILE}"ERROR)"
                sleep 1
                if [[ $(((success_status + error_status))) != "${TOTAL}" ]]; then
                    printf '%s\r' "$(_print_center "justify" "Status" ": ${success_status:-0} Downloaded | ${error_status:-0} Failed" "=")"
                fi
                TOTAL="$(((success_status + error_status)))"
            done
            _newline "\n"
            success_status="$(_count < "${TMPFILE}"SUCCESS)"
            error_status="$(_count < "${TMPFILE}"ERROR)"
            _clear_line 1 && _newline "\n"
        else
            while read -r -u 4 line; do
                _download_file_main parse "${line}"
                : "$((RETURN_STATUS < 2 ? (success_status += 1) : (error_status += 1)))"
                if [[ -z ${VERBOSE} ]]; then
                    for _ in 1 2 3 4; do _clear_line 1; done
                fi
                _print_center "justify" "Status" ": ${success_status:-0} Downloaded | ${error_status:-0} Failed" "="
            done 4<<< "${files_list}"
        fi
    fi

    for _ in 1 2; do _clear_line 1; done
    [[ ${success_status} -gt 0 ]] && "${QUIET:-_print_center}" "justify" "Downloaded" ": ${success_status}" "="
    [[ ${error_status} -gt 0 ]] && "${QUIET:-_print_center}" "justify" "Failed" ": ${error_status}" "="
    _newline "\n"

    if [[ -z ${SKIP_SUBDIRS} && -n ${num_of_folders} ]]; then
        while read -r -u 4 line; do
            (_download_folder "${line%%"|:_//_:|"*}" "${line##*"|:_//_:|"}" "${parallel:-}")
        done 4<<< "${folders_list}"
    fi

    cd - 2>| /dev/null 1>&2 || exit 1
    return 0
}

###################################################
# Log downloaded file info in case of -l / --log flag
# Todo: write doc
###################################################
_log_in_file() {
    [[ -z ${LOG_FILE_ID} || -d ${LOG_FILE_ID} ]] && return 0
    # shellcheck disable=SC2129
    # https://github.com/koalaman/shellcheck/issues/1202#issuecomment-608239163
    {
        printf "%s\n" "Name: ${1}"
        printf "%s\n" "Size: ${2}"
        printf "%s\n\n" "ID: ${3}"
    } >> "${LOG_FILE_ID}"
}
